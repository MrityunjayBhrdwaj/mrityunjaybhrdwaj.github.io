<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-04-12T20:44:46+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Learn Machine Learning The fun way</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</subtitle><author><name>Interactive Machine Learning</name></author><entry><title type="html">Bayesian Linear Regression</title><link href="http://localhost:4000/machine/learning/bayesianLinearRegression.html" rel="alternate" type="text/html" title="Bayesian Linear Regression" /><published>2020-04-12T00:00:00+05:30</published><updated>2020-04-12T00:00:00+05:30</updated><id>http://localhost:4000/machine/learning/bayesianLinearRegression</id><content type="html" xml:base="http://localhost:4000/machine/learning/bayesianLinearRegression.html">&lt;h2 id=&quot;what-is-bayesian-linear-regression-and-why-should-i-care&quot;&gt;What is Bayesian Linear Regression and why should I care?&lt;/h2&gt;

&lt;p&gt;Just like in simple linear regression in bayesian Linear Regression we try to find the best possible curve to fit to our &lt;span class=&quot;dataPoints&quot;&gt;data&lt;/span&gt; but along with that, we also take into account &lt;strong&gt;the uncertainity&lt;/strong&gt; towards our prediction. for example, in this visualization, we can see that in the areas where we have the &lt;span class=&quot;dataPoints&quot;&gt;data points&lt;/span&gt;, the width of the &lt;span class=&quot;stdev&quot;&gt;dashed curve&lt;/span&gt; is small which means our model is really confident about its prediction (which is showen in the &lt;span class=&quot;prediction&quot;&gt;solid red line&lt;/span&gt;) but if you see in the regions inwhich we don’t have any data points, the width of those &lt;span class=&quot;stdev&quot;&gt;dashed curve&lt;/span&gt; are much larger which inturn means, that the model is highly uncertain about its prediction…. but why should we care about modelling uncertainity? Suppose, you want to diagnose a patient and given the symptoms you want to predict how serious this patient is and should it require immidiate attention on not… because we know how confident our model is about its prediction, we can make better decisions like if its highly uncertain, then we should better consult with the expert or something and don’t believe on its predictions, whereas in the standard linear regression/ non-bayesian methods we can’t do any of that, there we only have a prediction and thats it… using bayesian linear regression, we can also find missing data and even learns what would be the best possible degree of the polynomial to fit to this data without having to manually set them( like we usually do)…&lt;/p&gt;</content><author><name>Mrityunjay</name></author><category term="ML" /><category term="AI" /><category term="linear-regression" /><category term="bayesian" /><category term="bayes" /><summary type="html">What is Bayesian Linear Regression and why should I care?</summary></entry><entry><title type="html">Understanding Recurrent Neural Nets</title><link href="http://localhost:4000/blog/artificial-intelligence/machinelearning/rnns.html" rel="alternate" type="text/html" title="Understanding Recurrent Neural Nets" /><published>2019-01-13T00:00:00+05:30</published><updated>2019-01-13T00:00:00+05:30</updated><id>http://localhost:4000/blog/artificial-intelligence/machinelearning/rnns</id><content type="html" xml:base="http://localhost:4000/blog/artificial-intelligence/machinelearning/rnns.html">&lt;p&gt;Recurrent Neural Nets are one of the most crucial architecture to understand in the field of deep learning and for good reason, today, when it comes to modeling sequential data, our goto architecture are RNNs, whether it’s predicting what are you going to type next or to have a virtual assistant like Siri or Alexa or even to pridict stock prices.in-short,RNNs are important.&lt;/p&gt;

&lt;p&gt;Now that you are convinced! we can move forward into really understanding it, don’t worry its not that unfamiliar…. it’s just a simple Neural-net including aspects of a dynamic system that are incorporated into its architecture with a twist in its backpropagation algorithm… so let’s get into it!&lt;/p&gt;

&lt;p&gt;As discussed before, RNNs are used to model sequential data which are usually generated by a dynamic system like our speech, we can think of a dynamic system(see equation (1)) as a function which depends on the function of previous time step… i.e, when we deliver a sentence our choice of current word is bounded by the choice of our previous word and so on.. for example, we can’t just say “Today harry a beautiful day” instead of “Today is a beautiful day”…&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S_t = f(S_{t-1}) \tag{1}&lt;/script&gt;

&lt;p&gt;and if we were to represent it in computation graph it looks something like fig 1.1 (a):&lt;/p&gt;

&lt;p class=&quot;text-center text-center&quot;&gt;&lt;img src=&quot;/assets/img/posts_imgs/rnn_with_math/body/rnn_dsys2unfold.jpg&quot; /&gt;
&lt;!-- &lt;i style=&quot;font-size:15px&quot;&gt;image source: WildML&lt;/i&gt; --&gt;
&lt;i style=&quot;font-size:15px&quot;&gt;fig 1.1&lt;/i&gt;
&lt;i style=&quot;font-size:15px&quot;&gt;a. Dynamic System (left) b. unfolded RNN graph (right)&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;Some Notations to be aware of:-&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;s_t&lt;/script&gt; = State or hidden units at time step &lt;script type=&quot;math/tex&quot;&gt;(t)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;o_t&lt;/script&gt; = output at time step (t)&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; = Weight Matrix b/w hidden unit and the output&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt; = Weight Matrix b/w hidden units&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; = Weight Matrix b/w input and &lt;script type=&quot;math/tex&quot;&gt;s_t&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;where,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_t = b + Ws_{t-1} + Ux_t&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;s_t = \sigma(a_t)&lt;/script&gt;   (where, &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt; = activation function )&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;o_t = c + Vs_t&lt;/script&gt;

&lt;p&gt;now, in order to predict the next word or the next stock prices we first need to input some data and the way we are going to be doing that is by putting every input-unit(word in a sentence,history of stock prices) into each time step,think of it like our&lt;a id=&quot;post_link&quot; target=&quot;_blank&quot; href=&quot;https://en.wikipedia.org/wiki/Prior_probability&quot;&gt; prior knowledge&lt;/a&gt; that we feed into our network in order to predict the next unit in the sequence…&lt;/p&gt;

&lt;p&gt;in case of speech generation, the output is a huge matrix consist of the probabilities of occurence of every word in our vocabulary,in case of stock market prediction its going to be just a simple output of prices…. from now on we are just going to be focusing on the general form of a simple RNNs&lt;/p&gt;

&lt;p&gt;ok, so we know how are we going to get our prediction but what if we predict it wrong?? first of all we need a measure to calculate the correctness of our prediction.enter, Loss function, loss function is just a accumulation of all the individual loss at each time step… i.e, &lt;script type=&quot;math/tex&quot;&gt;l_t = 1/2*(o_t - y_t)^2&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;y_t&lt;/script&gt; is the true output and &lt;script type=&quot;math/tex&quot;&gt;o_t&lt;/script&gt; is the probability @ timestep &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; although usually we use cross-entropy loss for language modelling but for simplicity we are going with standard&lt;a id=&quot;post_link&quot; target=&quot;_blank&quot; href=&quot;https://en.wikipedia.org/wiki/Mean_squared_error&quot;&gt; MSE&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;now that we know hot to quantify the correctness our prediction, we need to find a way to minimize it and just like a simple perceptron, we need to come up with Weights that minimize our Loss function.
and the way we are going to be doing that is by calculating the derivative of our Loss function w.r.t. these weights which will tells us in which direction we should move in order to minimize our Loss function and update our weights accordingly.
which means, we need to calculate these values:-&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial L}\over{\partial V} },
 { {\partial L}\over{\partial W} }, 
 { {\partial L}\over{\partial U} } \tag{2}&lt;/script&gt;

&lt;p&gt;but in order to calculate these, we first need to calculate some derivatives beforehand,which will prove useful in calculating our final weight derivatives(2). first we need to calculate the derivative of &lt;script type=&quot;math/tex&quot;&gt;o_t&lt;/script&gt; w.r.t our loss function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ { \partial L }\over{\partial o_t}} = { { \partial L }\over{ \partial l_t }} * { { \partial l_t }\over{ \partial o_t } }&lt;/script&gt;

&lt;p&gt;so as we know:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L = l_t + l_{t-1} + ..... + l_{1}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
{ { \partial L }\over{\partial l_t} } &amp;= { {\partial }\over{\partial l_t} }(l_t + l_{t-1} + ....... + l_1) \\
&amp;= 1
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;for second term :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
{ { \partial l_t } \over {\partial o_t} }  &amp;= {1\over2}({ {\partial } \over {\partial o_t} }((o_t - y_t)^2)) \\
&amp;=  (o_t - y_t)*1
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;putting it all together we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ { \partial L }\over {\partial o_t} } = { {(o_t-y_1)*1}*{1} }&lt;/script&gt;

&lt;p&gt;now the next thing we need to calculate is &lt;script type=&quot;math/tex&quot;&gt;{ {\partial L} \over{\partial s_t}}&lt;/script&gt;,
this is not an easy fleet… beacuse if you change &lt;script type=&quot;math/tex&quot;&gt;s_t&lt;/script&gt; its going to change} &lt;script type=&quot;math/tex&quot;&gt;s_{t+1}&lt;/script&gt; and} &lt;script type=&quot;math/tex&quot;&gt;s_{t+2}&lt;/script&gt; and so on… untill the end of the time or untill the end of our input sequence and finally change our Loss function. so our derivative is going to be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial L} \over{\partial s_t}} = ({ {\partial L} \over {\partial o_t} } * { {\partial o_t} \over {\partial s_t} }) + ({ {\partial L}\over{\partial s_{t+1}} } * { {\partial s_{t+1}}\over {\partial s_t} })&lt;/script&gt;

&lt;p&gt;for convenience, let &lt;script type=&quot;math/tex&quot;&gt;\kappa = { {\partial L} \over{\partial s} }&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\kappa_t = ({ {\partial L} \over {\partial o_t} } * { {\partial o_t} \over {\partial s_t} } ) + (\kappa_{t+1} * ({ {\partial L}\over{\partial s_{t+1}} }*{ {\partial s_{t+1}}\over {\partial s_t} }) )&lt;/script&gt;

&lt;p&gt;where, &lt;script type=&quot;math/tex&quot;&gt;{ {\partial s_{t+1}}\over {\partial s_t} } = { {\partial }\over {\partial s_t} }(s_t*W) = W&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;but we can’t calculate &lt;script type=&quot;math/tex&quot;&gt;\kappa_t&lt;/script&gt; just yet, basically we want to calcaulate the derivative w.r.t current time but as u can see in the expression above we also need to calculate the derivative w.r.t the next time step and so on…(because of &lt;script type=&quot;math/tex&quot;&gt;\kappa_{t+1}&lt;/script&gt; ) untill the end of time…(litrally)
that is why we first need to calculate the derivative w.r.t last time-step and then &lt;strong&gt;backpropagate through time&lt;/strong&gt; to get to the current time step (t), which is the reason why this algorithm is known as backpropagation through time(BPTT).&lt;/p&gt;

&lt;p&gt;so the derivative w.r.t last time step &lt;script type=&quot;math/tex&quot;&gt;(T)&lt;/script&gt; is going to be:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial L} \over{\partial s_T}} = { {\partial L} \over {\partial o_T} } * { {\partial o_T} \over {\partial s_T} }&lt;/script&gt;

&lt;p&gt;here, we know &lt;script type=&quot;math/tex&quot;&gt;{ {\partial L} \over {\partial o_T} }&lt;/script&gt;… but what about the second term??&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial o_T} \over {\partial s_T} }={ {\partial} \over {\partial s_T} }(c + Vs_T) = V&lt;/script&gt;

&lt;p&gt;our final &lt;script type=&quot;math/tex&quot;&gt;{ {\partial L} \over{\partial s_T}}&lt;/script&gt; :-&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial L} \over{\partial s_T}} = (o_t-y_t) * V&lt;/script&gt;

&lt;p&gt;it should be clear by now that the derivative of loss function w.r.t s_t is going to depend on the derivative w.r.t s_t+1 and s_t+2 and so on untill the last time step which we now know how to calculate, so our final expression of calculating &lt;script type=&quot;math/tex&quot;&gt;\kappa_t&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\kappa_t = ( (o_t - y_t)* V) + ((\kappa_{t+1}) * W )&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\kappa_{t+1} = ( (o_t - y_t)* V) + ((\kappa_{t+2}) * W )&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vdots&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\kappa_{T-1} = ( (o_t - y_t)* V) + ((\kappa_{T}) * W )&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\kappa_T = ( (o_t - y_t)* V)&lt;/script&gt;

&lt;p&gt;Finally we have every thing we need in order to calculate the derivatives our weights.. we can now move forward and calculate them respectively:-&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial L} \over{\partial V}} = { {\partial L} \over{\partial o_t} } * { {\partial o_t} \over{\partial V} }&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial L} \over{\partial W}} = { {\partial L} \over{\partial s_t} } * { {\partial s_t} \over{\partial W} }&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial L} \over{\partial U}} = { {\partial L} \over{\partial s_t} } * { {\partial s_t} \over{\partial U} }&lt;/script&gt;

&lt;p&gt;calculating,&lt;script type=&quot;math/tex&quot;&gt;{ {\partial L} \over{\partial V}}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
{ {\partial L} \over{\partial V}} &amp;= { {\partial L} \over{\partial o_t} } * { {\partial o_t} \over{\partial V} } \\
&amp;= (o_t - y_t) * { {\partial o_t} \over{\partial V} }
\end{align} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial o_t} \over{\partial V} } = { {\partial} \over{\partial V} }(V*s_t) = s_t&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial L} \over{\partial V}} = (o_t - y_t) * s_t&lt;/script&gt;

&lt;p&gt;calculating,&lt;script type=&quot;math/tex&quot;&gt;{ {\partial L} \over{\partial U} }&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
{ {\partial L} \over{\partial U}} &amp;= { {\partial L} \over{\partial s_t} } * { {\partial s_t} \over{\partial U} } \\
&amp;= (\kappa_t) * { {\partial s_t} \over{\partial U} }
\end{align} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial s_t} \over{\partial U} }
= { {\partial s_t} \over{\partial a_t} }*{ {\partial a_t} \over{\partial U} }&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial s_t} \over{\partial a_t} }
=  { {\partial } \over{\partial a_t} }(tanh(a_t))&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial s_t} \over{\partial a_t} }
=  (1-tanh^2(a_t))&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial a_t} \over{\partial U} }
= { {\partial } \over{\partial U} }(b+ W*s_{t-1} + U*x_t)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial a_t} \over{\partial U} }
= (x_t)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial s_t} \over{\partial U} }
= (1- tanh^2(a_t))*(x_t)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{   {\partial L} \over{\partial U}} = (\kappa_t)*((1-tanh^2(a_t)) * (x_{t}))&lt;/script&gt;

&lt;p&gt;calculating,&lt;script type=&quot;math/tex&quot;&gt;{ {\partial L} \over{\partial W}}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
{ {\partial L} \over{\partial W}} &amp;= { {\partial L} \over{\partial s_t} } * { {\partial s_t} \over{\partial W} } \\
&amp;= (\kappa_t) * { {\partial s_t} \over{\partial W} }
\end{align} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial s_t} \over{\partial W} }
= { {\partial s_t} \over{\partial a_t} }*{ {\partial a_t} \over{\partial W} }&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
{ {\partial a_t} \over{\partial W} } &amp;= { {\partial } \over{\partial W} }(b+ W*s_{t-1} + U*x_i) \\
&amp;= ( s_{t-1})
\end{align} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial s_t} \over{\partial W} }
= (1-tanh^2(a_t))* (s_{t-1})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial L} \over{\partial W}} = (\kappa_t)*((1-tanh^2(a_t)) * (s_{t-1}))&lt;/script&gt;

&lt;p&gt;putting all the derivatives together our final derivative of Loss function w.r.t all the weights are:-&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial L} \over{\partial V}} = (o_t - y_t) * s_t&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial L} \over{\partial U}} = (\kappa_t)*((1-tanh^2(a_t)) * (x_{t}))&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{ {\partial L} \over{\partial W}} = (\kappa_t)*((1-tanh^2(a_t)) * (s_{t-1}))&lt;/script&gt;

&lt;p&gt;Now that we have the derivatives of all the weights we can finally compute our weights&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V_{\text{new}} = V_{\text{old}} + \alpha{ {\partial L} \over{\partial V} }&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;W_{\text{new}} = W_{\text{old}} + \alpha{ {\partial L} \over{\partial W} }&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U_{\text{new}} = U_{\text{old}} + \alpha{ {\partial L} \over{\partial U} }&lt;/script&gt;

&lt;p&gt;Congratulations,now that you know how Recurrent Neural Network works internally/mathematically as well as intuitively…we can now worry about how to implement it… if you ask anyone in the industry they all say RNNs doesnt work as it is… in paper it is really great and this is how it works internall but in practice we need to do some modification in order to make it work and that we will do in the next part of this blog…&lt;/p&gt;</content><author><name>Mrityunjay</name></author><category term="ML" /><category term="DL" /><category term="AI" /><category term="RNN" /><category term="neural-networks" /><summary type="html">Recurrent Neural Nets are one of the most crucial architecture to understand in the field of deep learning and for good reason, today, when it comes to modeling sequential data, our goto architecture are RNNs, whether it’s predicting what are you going to type next or to have a virtual assistant like Siri or Alexa or even to pridict stock prices.in-short,RNNs are important.</summary></entry></feed>