<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-04-18T23:29:52+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Learn Machine Learning The fun way</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</subtitle><author><name>Machine Learning Playground</name></author><entry><title type="html">Principle Components Analysis</title><link href="http://localhost:4000/machine/learning/PCA.html" rel="alternate" type="text/html" title="Principle Components Analysis" /><published>2020-04-14T00:00:00+05:30</published><updated>2020-04-14T00:00:00+05:30</updated><id>http://localhost:4000/machine/learning/PCA</id><content type="html" xml:base="http://localhost:4000/machine/learning/PCA.html">&lt;!-- &lt;script&gt;
document.getElementById(&quot;myFrame&quot;).src = '/assets/viz//PCA/index.html'
&lt;/script&gt; --&gt;
&lt;h3 id=&quot;from-wiki-&quot;&gt;From Wiki:-&lt;/h3&gt;
&lt;p&gt;PCA can be thought of as fitting a p-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only an equally small amount of information.&lt;/p&gt;

&lt;p&gt;To find the axes of the ellipsoid, we must first subtract the mean of each variable from the dataset to center the data around the origin. Then, we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then we must normalize each of the orthogonal eigenvectors to become unit vectors. Once this is done, each of the mutually orthogonal, unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. This choice of basis will transform our covariance matrix into a diagonalised form with the diagonal elements representing the variance of each axis. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues.&lt;/p&gt;

&lt;p&gt;This procedure is sensitive to the scaling of the data, and there is no consensus as to how to best scale the data to obtain optimal results.&lt;/p&gt;</content><author><name>Mrityunjay</name></author><category term="ML" /><category term="AI" /><category term="dimensionality-reduction" /><category term="PCA" /><category term="unsupervised" /><summary type="html">From Wiki:- PCA can be thought of as fitting a p-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only an equally small amount of information.</summary></entry><entry><title type="html">Bayesian Linear Regression</title><link href="http://localhost:4000/machine/learning/bayesianLinearRegression.html" rel="alternate" type="text/html" title="Bayesian Linear Regression" /><published>2020-04-12T00:00:00+05:30</published><updated>2020-04-12T00:00:00+05:30</updated><id>http://localhost:4000/machine/learning/bayesianLinearRegression</id><content type="html" xml:base="http://localhost:4000/machine/learning/bayesianLinearRegression.html">&lt;h2 id=&quot;what-is-bayesian-linear-regression-and-why-should-i-care&quot;&gt;What is Bayesian Linear Regression and why should I care?&lt;/h2&gt;

&lt;p&gt;Just like in simple linear regression in bayesian Linear Regression we try to find the best possible curve to fit to our &lt;span class=&quot;dataPoints&quot;&gt;data&lt;/span&gt; but along with that, we also take into account &lt;strong&gt;the uncertainity&lt;/strong&gt; towards our prediction. for example, in this visualization, we can see that in the areas where we have the &lt;span class=&quot;dataPoints&quot;&gt;data points&lt;/span&gt;, the width of the &lt;span class=&quot;stdev&quot;&gt;dashed curve&lt;/span&gt; is small which means our model is really confident about its prediction (which is showen in the &lt;span class=&quot;prediction&quot;&gt;solid red line&lt;/span&gt;) but if you see in the regions inwhich we don’t have any data points, the width of those &lt;span class=&quot;stdev&quot;&gt;dashed curve&lt;/span&gt; are much larger which inturn means, that the model is highly uncertain about its prediction…. but why should we care about modelling uncertainity? Suppose, you want to diagnose a patient and given the symptoms you want to predict how serious this patient is and should it require immidiate attention on not… because we know how confident our model is about its prediction, we can make better decisions like if its highly uncertain, then we should better consult with the expert or something and don’t believe on its predictions, whereas in the standard linear regression/ non-bayesian methods we can’t do any of that, there we only have a prediction and thats it… using bayesian linear regression, we can also find missing data and even learns what would be the best possible degree of the polynomial to fit to this data without having to manually set them( like we usually do)…&lt;/p&gt;</content><author><name>Mrityunjay</name></author><category term="ML" /><category term="AI" /><category term="linear-regression" /><category term="bayesian" /><category term="bayes" /><summary type="html">What is Bayesian Linear Regression and why should I care?</summary></entry></feed>