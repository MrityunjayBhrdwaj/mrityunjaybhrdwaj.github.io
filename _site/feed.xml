<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-06-14T17:07:25+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Learn Machine Learning The fun way</title><subtitle>A platform for &lt;b&gt;Playing&lt;/b&gt;, &lt;b&gt;Exploring&lt;/b&gt; and &lt;b&gt;Understanding&lt;/b&gt; &lt;br&gt; several Machine Learning Algorithms.
</subtitle><author><name>Machine Learning Playgrounds</name></author><entry><title type="html">Linear Regression</title><link href="http://localhost:4000/machine/learning/linearRegression.html" rel="alternate" type="text/html" title="Linear Regression" /><published>2020-07-28T00:00:00+05:30</published><updated>2020-07-28T00:00:00+05:30</updated><id>http://localhost:4000/machine/learning/linearRegression</id><content type="html" xml:base="http://localhost:4000/machine/learning/linearRegression.html">&lt;p&gt;From Wiki:-&lt;/p&gt;

&lt;p&gt;In statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.[1] This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.[2]&lt;/p&gt;

&lt;p&gt;In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models.[3] Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.&lt;/p&gt;

&lt;p&gt;Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications.[4] This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.&lt;/p&gt;</content><author><name>Mrityunjay</name></author><category term="ML" /><category term="AI" /><category term="supervised" /><category term="linear" /><category term="linear Regression" /><summary type="html">From Wiki:-</summary></entry><entry><title type="html">Gaussian Processes</title><link href="http://localhost:4000/machine/learning/gaussianProcesses.html" rel="alternate" type="text/html" title="Gaussian Processes" /><published>2020-06-24T00:00:00+05:30</published><updated>2020-06-24T00:00:00+05:30</updated><id>http://localhost:4000/machine/learning/gaussianProcesses</id><content type="html" xml:base="http://localhost:4000/machine/learning/gaussianProcesses.html">&lt;p&gt;In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.&lt;/p&gt;</content><author><name>Mrityunjay</name></author><category term="ML" /><category term="AI" /><category term="supervised" /><category term="Kernels" /><category term="linear Regression" /><category term="Gaussian" /><category term="Gaussian Processes" /><category term="non-linear" /><category term="RKHS" /><summary type="html">In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.</summary></entry><entry><title type="html">AutoEncoder</title><link href="http://localhost:4000/machine/learning/autoEncoder.html" rel="alternate" type="text/html" title="AutoEncoder" /><published>2020-05-20T00:00:00+05:30</published><updated>2020-05-20T00:00:00+05:30</updated><id>http://localhost:4000/machine/learning/autoEncoder</id><content type="html" xml:base="http://localhost:4000/machine/learning/autoEncoder.html">&lt;h2 id=&quot;from-wiki-&quot;&gt;from wiki:-&lt;/h2&gt;

&lt;p&gt;An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. Several variants exist to the basic model, with the aim of forcing the learned representations of the input to assume useful properties.&lt;/p&gt;</content><author><name>Mrityunjay</name></author><category term="ML" /><category term="AI" /><category term="dimensionality-reduction" /><category term="AE" /><category term="supervised" /><category term="neural-network" /><category term="MNIST" /><summary type="html">from wiki:-</summary></entry><entry><title type="html">Support Vector Machine</title><link href="http://localhost:4000/machine/learning/svm.html" rel="alternate" type="text/html" title="Support Vector Machine" /><published>2020-05-15T00:00:00+05:30</published><updated>2020-05-15T00:00:00+05:30</updated><id>http://localhost:4000/machine/learning/svm</id><content type="html" xml:base="http://localhost:4000/machine/learning/svm.html">&lt;!-- &lt;script&gt;
document.getElementById(&quot;myFrame&quot;).src = '/assets/viz/SVM/index.html'
&lt;/script&gt; --&gt;
&lt;h3 id=&quot;from-wiki-&quot;&gt;From Wiki:-&lt;/h3&gt;
&lt;p&gt;In machine learning, support-vector machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap on which they fall.&lt;/p&gt;</content><author><name>Mrityunjay</name></author><category term="ML" /><category term="AI" /><category term="classification" /><category term="SVM" /><category term="supervised" /><category term="Vapnik" /><summary type="html">From Wiki:- In machine learning, support-vector machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap on which they fall.</summary></entry><entry><title type="html">Principle Components Analysis</title><link href="http://localhost:4000/machine/learning/PCA.html" rel="alternate" type="text/html" title="Principle Components Analysis" /><published>2020-04-14T00:00:00+05:30</published><updated>2020-04-14T00:00:00+05:30</updated><id>http://localhost:4000/machine/learning/PCA</id><content type="html" xml:base="http://localhost:4000/machine/learning/PCA.html">&lt;!-- &lt;script&gt;
document.getElementById(&quot;myFrame&quot;).src = '/assets/viz/PCA/index.html'
&lt;/script&gt; --&gt;
&lt;h3 id=&quot;from-wiki-&quot;&gt;From Wiki:-&lt;/h3&gt;
&lt;p&gt;PCA can be thought of as fitting a p-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only an equally small amount of information.&lt;/p&gt;

&lt;p&gt;To find the axes of the ellipsoid, we must first subtract the mean of each variable from the dataset to center the data around the origin. Then, we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then we must normalize each of the orthogonal eigenvectors to become unit vectors. Once this is done, each of the mutually orthogonal, unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. This choice of basis will transform our covariance matrix into a diagonalised form with the diagonal elements representing the variance of each axis. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues.&lt;/p&gt;

&lt;p&gt;This procedure is sensitive to the scaling of the data, and there is no consensus as to how to best scale the data to obtain optimal results.&lt;/p&gt;</content><author><name>Mrityunjay</name></author><category term="ML" /><category term="AI" /><category term="dimensionality-reduction" /><category term="PCA" /><category term="unsupervised" /><summary type="html">From Wiki:- PCA can be thought of as fitting a p-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only an equally small amount of information.</summary></entry><entry><title type="html">Bayesian Linear Regression</title><link href="http://localhost:4000/machine/learning/bayesianLinearRegression.html" rel="alternate" type="text/html" title="Bayesian Linear Regression" /><published>2020-04-12T00:00:00+05:30</published><updated>2020-04-12T00:00:00+05:30</updated><id>http://localhost:4000/machine/learning/bayesianLinearRegression</id><content type="html" xml:base="http://localhost:4000/machine/learning/bayesianLinearRegression.html">&lt;h2 id=&quot;what-is-bayesian-linear-regression-and-why-should-i-care&quot;&gt;What is Bayesian Linear Regression and why should I care?&lt;/h2&gt;

&lt;p&gt;Just like in simple linear regression in bayesian Linear Regression we try to find the best possible curve to fit to our &lt;span class=&quot;dataPoints&quot;&gt;data&lt;/span&gt; but along with that, we also take into account &lt;strong&gt;the uncertainity&lt;/strong&gt; towards our prediction. for example, in this visualization, we can see that in the areas where we have the &lt;span class=&quot;dataPoints&quot;&gt;data points&lt;/span&gt;, the width of the &lt;span class=&quot;stdev&quot;&gt;dashed curve&lt;/span&gt; is small which means our model is really confident about its prediction (which is showen in the &lt;span class=&quot;prediction&quot;&gt;solid red line&lt;/span&gt;) but if you see in the regions inwhich we don’t have any data points, the width of those &lt;span class=&quot;stdev&quot;&gt;dashed curve&lt;/span&gt; are much larger which inturn means, that the model is highly uncertain about its prediction…. but why should we care about modelling uncertainity? Suppose, you want to diagnose a patient and given the symptoms you want to predict how serious this patient is and should it require immidiate attention on not… because we know how confident our model is about its prediction, we can make better decisions like if its highly uncertain, then we should better consult with the expert or something and don’t believe on its predictions, whereas in the standard linear regression/ non-bayesian methods we can’t do any of that, there we only have a prediction and thats it… using bayesian linear regression, we can also find missing data and even learns what would be the best possible degree of the polynomial to fit to this data without having to manually set them( like we usually do)…&lt;/p&gt;</content><author><name>Mrityunjay</name></author><category term="ML" /><category term="AI" /><category term="linear-regression" /><category term="bayesian" /><category term="bayes" /><summary type="html">What is Bayesian Linear Regression and why should I care?</summary></entry></feed>