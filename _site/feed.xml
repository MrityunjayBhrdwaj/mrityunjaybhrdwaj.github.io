<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-07-20T15:06:21+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Learn Machine Learning The fun way</title><subtitle>A platform for &lt;b&gt;Playing&lt;/b&gt;, &lt;b&gt;Exploring&lt;/b&gt; and &lt;b&gt;Understanding&lt;/b&gt; &lt;br&gt; several Machine Learning Algorithms.
</subtitle><author><name>Machine Learning Playgrounds</name></author><entry><title type="html">Linear Regression</title><link href="http://localhost:4000/machine/learning/linearRegression.html" rel="alternate" type="text/html" title="Linear Regression" /><published>2020-07-28T00:00:00+05:30</published><updated>2020-07-28T00:00:00+05:30</updated><id>http://localhost:4000/machine/learning/linearRegression</id><content type="html" xml:base="http://localhost:4000/machine/learning/linearRegression.html">&lt;h2 id=&quot;um-what-am-i-looking-at&quot;&gt;Um, what am i looking at?&lt;/h2&gt;

&lt;p&gt;This is a simple linear regression which tries to fit the best possible line from the given data points.&lt;/p&gt;

&lt;p&gt;Just tap on the grid(Data Space) to add a new data points ( you can also drag the data points by click and drag),  play with the controls on the right and hit recalculate!&lt;/p&gt;

&lt;h2 id=&quot;hyperparameters&quot;&gt;Hyperparameters:&lt;/h2&gt;

&lt;p&gt;Epoch: for how long should we run the gradient descent iteraction.&lt;/p&gt;

&lt;p&gt;Learning rate: how much we scale our gradient at each time step to correct our model.&lt;/p&gt;

&lt;h2 id=&quot;but-what-is-linear-regression&quot;&gt;But, What is Linear Regression?&lt;/h2&gt;

&lt;p&gt;The goal of this method is to determine the linear model that minimizes the sum of the squared errors between the observations in a dataset and those predicted by the model.&lt;/p&gt;

&lt;!-- TODO: add viz of sequared error --&gt;
&lt;!-- we can visualize these sequared errors by simply clicking on 'view squared error button' as well as see our SSE decreasing overtime in the top right loss plot. --&gt;

&lt;!-- TODO: write colab notebook on implementing LR from scratch. --&gt;
&lt;!-- If you are someone who learns by doing, then please checkout our tutorial on how to implement Linear Regression from scratch! --&gt;

&lt;h2 id=&quot;further-reading&quot;&gt;Further reading:&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://en.m.wikipedia.org/wiki/Linear_regression#:~:text=In%20statistics%2C%20linear%20regression%20is,as%20dependent%20and%20independent%20variables.&amp;amp;text=Such%20models%20are%20called%20linear%20models.&quot;&gt;Wiki: Linear Regression&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://youtu.be/nk2CQITm_eo&quot;&gt;StatQuest: Linear Regression&lt;/a&gt;&lt;/p&gt;</content><author><name>Mrityunjay</name></author><category term="ML" /><category term="AI" /><category term="supervised" /><category term="linear" /><category term="linear Regression" /><summary type="html">Um, what am i looking at?</summary></entry><entry><title type="html">Gaussian Processes</title><link href="http://localhost:4000/machine/learning/gaussianProcesses.html" rel="alternate" type="text/html" title="Gaussian Processes" /><published>2020-06-24T00:00:00+05:30</published><updated>2020-06-24T00:00:00+05:30</updated><id>http://localhost:4000/machine/learning/gaussianProcesses</id><content type="html" xml:base="http://localhost:4000/machine/learning/gaussianProcesses.html">&lt;h2 id=&quot;um-what-am-i-looking-at&quot;&gt;Um, what am i looking at?&lt;/h2&gt;

&lt;p&gt;This is a simple Gaussian Process regression visualization, which tries to fit the best possible curve from the given data points.&lt;/p&gt;

&lt;p&gt;Just tap on the grid(Data Space) to add a new data points ( you can also drag the data points by click and drag),  play with the controls on the right and hit Recalculate!&lt;/p&gt;

&lt;h2 id=&quot;but-what-is-a-gaussian-process&quot;&gt;But, What is a Gaussian Process?&lt;/h2&gt;

&lt;h3 id=&quot;from-wiki&quot;&gt;From Wiki:&lt;/h3&gt;
&lt;p&gt;In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.&lt;/p&gt;

&lt;h2 id=&quot;further-reading&quot;&gt;Further reading:&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Gaussian_process&quot;&gt;Wiki: Gaussian Process&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=92-98SYOdlY&quot;&gt;MLSS 2017:  Gaussian Processes (Richard Turner)&lt;/a&gt;&lt;/p&gt;</content><author><name>Mrityunjay</name></author><category term="ML" /><category term="AI" /><category term="supervised" /><category term="Kernels" /><category term="linear Regression" /><category term="Gaussian" /><category term="Gaussian Processes" /><category term="non-linear" /><category term="RKHS" /><summary type="html">Um, what am i looking at?</summary></entry><entry><title type="html">AutoEncoder</title><link href="http://localhost:4000/machine/learning/autoEncoder.html" rel="alternate" type="text/html" title="AutoEncoder" /><published>2020-05-20T00:00:00+05:30</published><updated>2020-05-20T00:00:00+05:30</updated><id>http://localhost:4000/machine/learning/autoEncoder</id><content type="html" xml:base="http://localhost:4000/machine/learning/autoEncoder.html">&lt;h2 id=&quot;um-what-am-i-looking-at&quot;&gt;Um, what am i looking at?&lt;/h2&gt;

&lt;p&gt;This is a visualization of An Autoencoder, which tries to encode the inputs into a lower dimensional representation without sacrificing the reconstruction accuracy after decoding.&lt;/p&gt;

&lt;p&gt;Here we are using a pretrained Autoencoder which is trained on MNIST Dataset. you can see how a particular image of 784 dim is being encoded in just 2-dim by clicking ‘get random image’ button.&lt;/p&gt;

&lt;p&gt;you can also walk inside the latent space grid and see how that 2d purple point is being decoded as an image in the end by simply, dragging and dropping that point and seeing the magic ;).&lt;/p&gt;

&lt;h2 id=&quot;but-what-is-an-autoencoder&quot;&gt;But, What is an Autoencoder?&lt;/h2&gt;
&lt;p&gt;from wiki:-&lt;/p&gt;

&lt;p&gt;An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. Several variants exist to the basic model, with the aim of forcing the learned representations of the input to assume useful properties.&lt;/p&gt;

&lt;h2 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Autoencoder&quot;&gt;Wiki: Autoencoder&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Rdpbnd0pCiI&quot;&gt;two minutes paper: What is an Autoencoder&lt;/a&gt;&lt;/p&gt;</content><author><name>Mrityunjay</name></author><category term="ML" /><category term="AI" /><category term="dimensionality-reduction" /><category term="AE" /><category term="supervised" /><category term="neural-network" /><category term="MNIST" /><summary type="html">Um, what am i looking at?</summary></entry><entry><title type="html">Support Vector Machine</title><link href="http://localhost:4000/machine/learning/svm.html" rel="alternate" type="text/html" title="Support Vector Machine" /><published>2020-05-15T00:00:00+05:30</published><updated>2020-05-15T00:00:00+05:30</updated><id>http://localhost:4000/machine/learning/svm</id><content type="html" xml:base="http://localhost:4000/machine/learning/svm.html">&lt;!-- &lt;script&gt;
document.getElementById(&quot;myFrame&quot;).src = '/assets/viz/SVM/index.html'
&lt;/script&gt; --&gt;

&lt;h2 id=&quot;um-what-am-i-looking-at&quot;&gt;Um, what am i looking at?&lt;/h2&gt;

&lt;p&gt;This is a visualization of Support Vector Machine, which tries to fit the best possible decision Boundary from the given data points.&lt;/p&gt;

&lt;p&gt;Just tap on the grid(Data Space) to add a new data points ( you can also drag the data points by click and drag), if you want to change the class of a new data point then simply click on the ‘Current Class’ switch.&lt;/p&gt;

&lt;p&gt;Go ahead, play with the controls on the right and have fun!&lt;/p&gt;

&lt;h3 id=&quot;from-wiki-&quot;&gt;From Wiki:-&lt;/h3&gt;
&lt;p&gt;In machine learning, support-vector machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap on which they fall.&lt;/p&gt;

&lt;h2 id=&quot;hyperparameters&quot;&gt;Hyperparameters:&lt;/h2&gt;

&lt;p&gt;C: Regularization term&lt;/p&gt;

&lt;p&gt;Kernel Type: Type of Kernel, currently support 3:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Linear:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x,y) = x^Ty&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Polynomial:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x,y) = (x^Ty + c)^d&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;RBF:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x,y) = exp(-\frac{||x - y||^2}{2\sigma^2})&lt;/script&gt;

&lt;p&gt;We also have Hyperparamters to control the behavior of our Kernels:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If we select Polynomial Kernel then we can choose the Degree of our polynomial upto 3.&lt;/li&gt;
  &lt;li&gt;If we select RBF Kernel then we can select the width of our kernel a.k.a sigma( &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Support-vector_machine&quot;&gt;Wiki: Support Vector Machine&lt;/a&gt;
&lt;a href=&quot;https://www.youtube.com/watch?v=efR1C6CvhmE&quot;&gt;StatQuest: Support Vector Machine&lt;/a&gt;&lt;/p&gt;</content><author><name>Mrityunjay</name></author><category term="ML" /><category term="AI" /><category term="classification" /><category term="SVM" /><category term="supervised" /><category term="Vapnik" /><summary type="html"></summary></entry><entry><title type="html">Principle Components Analysis</title><link href="http://localhost:4000/machine/learning/PCA.html" rel="alternate" type="text/html" title="Principle Components Analysis" /><published>2020-04-14T00:00:00+05:30</published><updated>2020-04-14T00:00:00+05:30</updated><id>http://localhost:4000/machine/learning/PCA</id><content type="html" xml:base="http://localhost:4000/machine/learning/PCA.html">&lt;!-- &lt;script&gt;
document.getElementById(&quot;myFrame&quot;).src = '/assets/viz/PCA/index.html'
&lt;/script&gt; --&gt;

&lt;h2 id=&quot;um-what-am-i-looking-at&quot;&gt;Um, what am i looking at?&lt;/h2&gt;

&lt;p&gt;This is a simple visualization of PCA, which tries to find the direction of maximum spreads of our data.
 &lt;!-- we can then project our data onto these directions(a.k.a principle components) to reduce the dimensionality without sacrificing data's defining characterstics. --&gt;&lt;/p&gt;

&lt;p&gt;Just tap on the left-most grid(a.k.a Data Space) to add a new data points ( you can also drag them! ) and see how your principle components(eigen vectors) and covariances(visualized as blueish green ellipse) are changing.&lt;/p&gt;

&lt;p&gt;If things gets slower, simply switch off the Live-Update and use Recalculate button instead.&lt;/p&gt;

&lt;!-- TODO: implement projection onto principle components --&gt;
&lt;h2 id=&quot;but-what-is-principle-components-analysis&quot;&gt;But, What is Principle Components Analysis?…&lt;/h2&gt;

&lt;p&gt;As mentioned earlier, Principle Components Analysis tries to find the direction of maximum spreads (a.k.a variance) of our data. To this end, we first need to calculate the covariance matrix of our &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_scaling&quot;&gt;standardized data&lt;/a&gt; then we can simply perform eigendecomposition onto them.&lt;/p&gt;

&lt;p&gt;the eigendecoposition spits out 2 things, eigenvectors and eigenvalues. as it is usually being defined in the literature, eigenvectors of our covariance matrix essentially gives us the directions and magnitude of maximum variances in dessending order. i.e, the first eigenvector is the direction of heighest spread of our data which is exactly what we want! also, eigenvalues characterizes the magnitude of our spread.&lt;/p&gt;

&lt;p&gt;By the way, the eigenvectors of our covariance matrix is also know as principle components of our data.
once we’ve calculated them, we can reduce the dimensionaly of our data by projecting our data (which is 2d-dim in our case) into one of the principle components, effectively explaining 50% of the variance in our data.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;a href=&quot;https://brilliant.org/wiki/principal-component-analysis/&quot;&gt;
	&lt;img src=&quot;http://localhost:4000/assets/img/posts_imgs/PCA/body/pca-proj.png&quot; /&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;!-- &lt;a href=&quot;https://builtin.com/data-science/step-step-explanation-principal-component-analysis&quot;&gt;
	&lt;img src=&quot;http://localhost:4000/assets/img/posts_imgs/PCA/body/pca.gif&quot;&gt;
&lt;/a&gt; --&gt;

&lt;p&gt;We can also employ this exact same technique to reduce the dimensionaly of data in real world for e.g. the image below is 2d representation of MNIST dataset inwhich each image is of 784-dimension.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;
&lt;a href=&quot;https://www.researchgate.net/figure/Projection-into-a-3D-space-via-PCA-of-the-MNIST-benchmark-dataset-This-data-set_fig3_261567034&quot;&gt;
	&lt;img src=&quot;http://localhost:4000/assets/img/posts_imgs/PCA/body/pca-MNIST-proj.png&quot; /&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;h2 id=&quot;further-reading&quot;&gt;Further reading&lt;/h2&gt;

&lt;p&gt;Yea i know, its &lt;strong&gt;alot&lt;/strong&gt; to take in, heck there is also a pretty famous meme for it:&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
&lt;a href=&quot;https://towardsdatascience.com/algorithms-from-scratch-pca-cde10b835ebc&quot;&gt;
	&lt;img src=&quot;http://localhost:4000/assets/img/posts_imgs/PCA/body/pca-meme.jpeg&quot; width=&quot;300px&quot; /&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;but if you want to learn more, there are plenty of great resources out there. some of which includes:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_component_analysis&quot;&gt;Wiki: Principal Components Analysis&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=FgakZw6K1QQ&quot;&gt;StatQuest:  PCA&lt;/a&gt;&lt;/p&gt;</content><author><name>Mrityunjay</name></author><category term="ML" /><category term="AI" /><category term="dimensionality-reduction" /><category term="PCA" /><category term="unsupervised" /><summary type="html"></summary></entry><entry><title type="html">Bayesian Linear Regression</title><link href="http://localhost:4000/machine/learning/bayesianLinearRegression.html" rel="alternate" type="text/html" title="Bayesian Linear Regression" /><published>2020-04-12T00:00:00+05:30</published><updated>2020-04-12T00:00:00+05:30</updated><id>http://localhost:4000/machine/learning/bayesianLinearRegression</id><content type="html" xml:base="http://localhost:4000/machine/learning/bayesianLinearRegression.html">&lt;h2 id=&quot;um-what-am-i-looking-at&quot;&gt;Um, what am i looking at?&lt;/h2&gt;

&lt;p&gt;This is a visualization of Bayesian Linear regression using Polynomial Kernel which tries to fit the best possible curve from the given data points.&lt;/p&gt;

&lt;p&gt;Just Play with different controls and Hit Recalculate to see how Best Fit Curve Behaves!&lt;/p&gt;

&lt;h2 id=&quot;but-what-is-bayesian-linear-regression&quot;&gt;But, What is Bayesian Linear Regression?&lt;/h2&gt;

&lt;p&gt;Just like in simple linear regression in bayesian Linear Regression we try to find the best possible curve to fit to our &lt;span class=&quot;dataPoints&quot;&gt;data&lt;/span&gt; but along with that, we also take into account &lt;strong&gt;the uncertainity&lt;/strong&gt; towards our prediction. for example, in this visualization, we can see that in the areas where we have the &lt;span class=&quot;dataPoints&quot;&gt;data points&lt;/span&gt;, the width of the &lt;span class=&quot;stdev&quot;&gt;dashed curve&lt;/span&gt; is small which means our model is really confident about its prediction (which is showen in the &lt;span class=&quot;prediction&quot;&gt;solid red line&lt;/span&gt;) but if you see in the regions inwhich we don’t have any data points, the width of those &lt;span class=&quot;stdev&quot;&gt;dashed curve&lt;/span&gt; are much larger which inturn means, that the model is highly uncertain about its prediction…. but why should we care about modelling uncertainity? Suppose, you want to diagnose a patient and given the symptoms you want to predict how serious this patient is and should it require immidiate attention on not… because we know how confident our model is about its prediction, we can make better decisions like if its highly uncertain, then we should better consult with the expert or something and don’t believe on its predictions, whereas in the standard linear regression/ non-bayesian methods we can’t do any of that, there we only have a prediction and thats it… using bayesian linear regression, we can also find missing data and even learns what would be the best possible degree of the polynomial to fit to this data without having to manually set them( like we usually do)…&lt;/p&gt;

&lt;h2 id=&quot;hyperparameters&quot;&gt;Hyperparameters&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Degree: Degree of the Polynomial Kernel&lt;/li&gt;
  &lt;li&gt;DataSize: No. of data points in our dataset.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_linear_regression&quot;&gt;Wiki: Bayesian Linear Regression&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=dtkGq9tdYcI&quot;&gt;mathematicalmonk: Bayesian Linear Regression&lt;/a&gt;&lt;/p&gt;</content><author><name>Mrityunjay</name></author><category term="ML" /><category term="AI" /><category term="linear-regression" /><category term="bayesian" /><category term="bayes" /><summary type="html">Um, what am i looking at?</summary></entry></feed>