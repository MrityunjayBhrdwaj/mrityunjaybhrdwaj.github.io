---
layout: post 
title : Principle Components Analysis
tags  : [ML,AI,dimensionality-reduction,PCA, unsupervised]
title-seprator: "|"
categories: Machine Learning
permalink: /:categories/:title.html
mathjax: true
author: Mrityunjay
img: /posts_imgs/PCA/teaser/pcaScrst.png
viz: PCA/index.html

---


<!-- <script>
document.getElementById("myFrame").src = '{{"/assets/viz/" | prepend: site.baseurl | append : page.viz}}'
</script> -->

## Um, what am i looking at?

This is a simple visualization of PCA, where we first standardize our data and then perform SVD which intransically gives us the principle components of the covariance matrix of our data which is visualized by the bluish ellipse.

these principle components are the directions where we get the biggest spread of our data.

Just tap on the grid(Data Space) to add a new data points ( you can also drag the data points by click and drag) and see how your principle components and covariance are changing.

## But, What is Principle Components Analysis?

### From Wiki:
PCA can be thought of as fitting a p-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only an equally small amount of information.

To find the axes of the ellipsoid, we must first subtract the mean of each variable from the dataset to center the data around the origin. Then, we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then we must normalize each of the orthogonal eigenvectors to become unit vectors. Once this is done, each of the mutually orthogonal, unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. This choice of basis will transform our covariance matrix into a diagonalised form with the diagonal elements representing the variance of each axis. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues.

This procedure is sensitive to the scaling of the data, and there is no consensus as to how to best scale the data to obtain optimal results.


## Further reading:
[Wiki: Principal Components Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)

[StatQuest:  PCA](https://www.youtube.com/watch?v=FgakZw6K1QQ)