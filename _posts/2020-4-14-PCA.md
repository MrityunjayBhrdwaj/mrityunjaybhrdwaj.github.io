---
layout: post 
title : Principle Components Analysis
tags  : [ML,AI,dimensionality-reduction,PCA, unsupervised]
title-seprator: "|"
categories: Machine Learning
permalink: /:categories/:title.html
mathjax: true
author: Mrityunjay
img: /posts_imgs/PCA/teaser/pcaScrst.png
viz: /PCA/index.html

---


<!-- <script>
document.getElementById("myFrame").src = '{{"/assets/viz/" | prepend: site.baseurl | append : page.viz}}'
</script> -->
### From Wiki:-
PCA can be thought of as fitting a p-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only an equally small amount of information.

To find the axes of the ellipsoid, we must first subtract the mean of each variable from the dataset to center the data around the origin. Then, we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then we must normalize each of the orthogonal eigenvectors to become unit vectors. Once this is done, each of the mutually orthogonal, unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. This choice of basis will transform our covariance matrix into a diagonalised form with the diagonal elements representing the variance of each axis. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues.

This procedure is sensitive to the scaling of the data, and there is no consensus as to how to best scale the data to obtain optimal results.

