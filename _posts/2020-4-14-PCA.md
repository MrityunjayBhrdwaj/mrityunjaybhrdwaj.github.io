---
layout: post 
title : Principle Components Analysis
tags  : [ML,AI,dimensionality-reduction,PCA, unsupervised]
title-seprator: "|"
categories: Machine Learning
permalink: /:categories/:title.html
mathjax: true
author: Mrityunjay
img: /posts_imgs/PCA/teaser/pcaScrst.png
viz: PCA/index.html

---


<!-- <script>
document.getElementById("myFrame").src = '{{"/assets/viz/" | prepend: site.baseurl | append : page.viz}}'
</script> -->

## Um, what am i looking at?

This is a simple visualization of PCA, which tries to find the direction of maximum spreads of our data.
 <!-- we can then project our data onto these directions(a.k.a principle components) to reduce the dimensionality without sacrificing data's defining characterstics. -->

Just tap on the left-most grid(a.k.a Data Space) to add a new data points ( you can also drag them! ) and see how your principle components(eigen vectors) and covariances(visualized as blueish green ellipse) are changing.

If things gets slower, simply switch off the Live-Update and use Recalculate button instead.

<!-- TODO: implement projection onto principle components -->
## But, What is Principle Components Analysis?

As mentioned earlier, Principle Components Analysis tries to find the direction of maximum spreads (a.k.a variance) of our data. To this end, we first need to calculate the covariance matrix of our [standardized data](https://en.wikipedia.org/wiki/Feature_scaling) then we can simply perform eigendecomposition onto them.

the eigendecoposition spits out 2 things, eigenvectors and eigenvalues. as it is usually being defined in the literature, eigenvectors of our covariance matrix essentially gives us the directions and magnitude of maximum variances in dessending order. i.e, the first eigenvector is the direction of heighest spread of our data which is exactly what we want! also, eigenvalues characterizes the magnitude of our spread.

By the way, the eigenvectors of our covariance matrix is also know as principle components of our data.
once we've calculated them, we can reduce the dimensionaly of our data by projecting our data (which is 2d-dim in our case) into one of the principle components, effectively explaining 50% of the variance in our data.

<div style="text-align:center">
<a href="https://brilliant.org/wiki/principal-component-analysis/">
	<img src="{{site.url}}/assets/img/posts_imgs/PCA/body/pca-proj.png">
</a>
</div>
<!-- <a href="https://builtin.com/data-science/step-step-explanation-principal-component-analysis">
	<img src="{{site.url}}/assets/img/posts_imgs/PCA/body/pca.gif">
</a> -->

We can also employ this exact same technique to reduce the dimensionaly of data in real world for e.g. the image below is 2d representation of MNIST dataset inwhich each image is of 784-dimension. 

<div style="text-align:center">
<a href="https://www.researchgate.net/figure/Projection-into-a-3D-space-via-PCA-of-the-MNIST-benchmark-dataset-This-data-set_fig3_261567034">
	<img src="{{site.url}}/assets/img/posts_imgs/PCA/body/pca-MNIST-proj.png">
</a>
</div>

## Further reading:
[Wiki: Principal Components Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)

[StatQuest:  PCA](https://www.youtube.com/watch?v=FgakZw6K1QQ)